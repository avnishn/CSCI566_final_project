<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0031)https://clvrai.github.io/spirl/ -->
<html><!-- ======================================================================= --><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script type="text/javascript" id="www-widgetapi-script" src="./demystifying_reproducibility_in_meta_mtrl_files/www-widgetapi.js" async=""></script><script src="./demystifying_reproducibility_in_meta_mtrl_files/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<style type="text/css">
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 100%;
  }

  h1 {
    font-weight:300;
  }

  div {
    max-width: 95%;
    margin:auto;
    padding: 10px;
  }

  .table-like {
    display: flex;
    flex-wrap: wrap;
    flex-flow: row wrap;
    justify-content: center;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img {
    padding: 0;
    display: block;
    margin: 0 auto;
    max-height: 100%;
    max-width: 100%;
  }

  iframe {
    max-width: 100%;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  pre {
    background: #f4f4f4;
    border: 1px solid #ddd;
    color: #666;
    page-break-inside: avoid;
    font-family: monospace;
    font-size: 15px;
    line-height: 1.6;
    margin-bottom: 1.6em;
    max-width: 100%;
    overflow: auto;
    padding: 10px;
    display: block;
    word-wrap: break-word;
}

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    max-width: 1100px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }
</style>
<!-- ======================================================================= -->

<!-- Start : Google Analytics Code -->
<!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-64069893-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-64069893-4');
</script> -->
<!-- End : Google Analytics Code -->

<script type="text/javascript" src="./demystifying_reproducibility_in_meta_mtrl_files/hidebib.js"></script>
<link href="./demystifying_reproducibility_in_meta_mtrl_files/css" rel="stylesheet" type="text/css">

</head><body><div max-width="100%">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="icon" type="image/png" href="https://clvrai.github.io/spirl/resources/clvr_icon.png">
  <title>Demystifying Reproducibility In Meta-RL and Multi-task-RL</title>
  <meta name="HandheldFriendly" content="True">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="canonical" href="https://avnishn.github.io/">
  <meta name="referrer" content="no-referrer-when-downgrade">

  <meta property="og:site_name" content="Demystifying Reproducibility In Meta-RL and Multi-task-RL">
  <meta property="og:type" content="video.other">
  <meta property="og:title" content="Demystifying Reproducibility In Meta-RL and Multi-task-RL">
  <meta property="og:description" content="Avnish Narayan, Demystifying Reproducibility In Meta-RL and Multi-task-RL. 2020.">
  <meta property="og:url" content="https://avnishn.github.io/">
  <meta property="og:image" content="https://github.com/clvrai/spirl/docs/resources/spirl_teaser.png">  <!-- UPDATE -->
  <!--<meta property="og:video" content="https://www.youtube.com/v/axXx-x86IeY" />   &lt;!&ndash; UPDATE &ndash;&gt;-->

  <meta property="article:publisher" content="https://avnishn.github.io/">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Demystifying Reproducibility In Meta-RL and Multi-task-RL">
  <meta name="twitter:description" content="Avnish Narayan. Demystifying Reproducibility In Meta-RL and Multi-task-RL. 2020.">
  <meta name="twitter:url" content="https://avnishn.github.io/">
  <meta property="og:image:width" content="3902">
  <meta property="og:image:height" content="1337">

  <script src="./demystifying_reproducibility_in_meta_mtrl_files/iframe_api"></script>
  <meta name="twitter:card" content="player">
  <meta name="twitter:player:width" content="640">
  <meta name="twitter:player:height" content="360">




      <br>
      <center><span style="font-size:44px;font-weight:bold;">Demystifying Reproducibility <br> In Meta-RL and Multi-task-RL</span></center><br>
      <div class="table-like" style="justify-content:space-evenly;max-width:600px;margin:auto;">
          <div><center><span style="font-size:30px"><a href="https://avnishn.github.io/" target="_blank">Avnish Narayan</a></span></center>
          <!-- <center><span style="font-size:18px">USC</span></center> -->
          </div>

          <!-- <center><span style="font-size:18px">UPenn</span></center>-->          
          </div>

          <!-- <center><span style="font-size:18px">UC Berkeley</span></center> -->
          </div>
      </div>
      <table align="center" width="30%" style="padding-top:0px;padding-bottom:0px">
          <tbody><tr>
            <td align="center"><center><span style="font-size:25px"><a href="https://www.clvrai.com/" target="_blank">CSCI 566, University of Southern California</a></span></center></td>
          </tr><tr>
      </tr></tbody></table>

      <div class="table-like" style="justify-content:space-evenly;max-width:500px;margin:auto;padding:5px">
        <div><center><span style="font-size:28px"><a href="https://github.com/rlworkgroup/garage">[GitHub Code]</a></span></center> </div>   <!-- UPDATE -->
      </div>

      <div style="width:800px; margin:0 auto;padding:5px" align="justify">
        Meta- and multi-task reinforcement learning (meta-RL and MTRL) show
        great promise for achieving long-term research goals like continual
        learning, and real-world applications like robotics, by making
        adaptation a first-class capability of intelligent agents.
        <p></p>Reproducibility in this field of research is a problem because:</p>
        <ul>
        <li><p>Evaluation of any methods requires strong quantitative analysis 
            for realizing true algorithm/method potential and seed sensitivity.</p>
        </li>
        <li><p>No 2 open-sourced implementations of any algorithm are the same.</p>
        </li>
        </ul>

      </div>
      <br><hr>

      <!-- ################### OVERVIEW #################### -->

      <center><h1>Overview</h1></center><div style="width:800px; margin:0 auto;padding:5px" align="justify">
        <img src="https://meta-world.github.io/img/metalearning.png" class="img-fluid" alt="">
        <p>In multi-task learning and meta-learning, the goal is not just to 
          learn one skill, but to learn a number of skills. 
        </p>
        <p>
          In multi-task RL, 
          we assume that we want to learn a fixed set of skills with minimal 
          data, while in meta-RL, we want to use experience from a set of 
          skills such that we can learn to solve new skills quickly.
        </p>

        <h3><strong>Can small differences in implementations of common multi-task and meta-RL algorithms make significant changes in algorithm performance?</strong></h3>
        
    <br><hr>


    <!-- ################### ENVIRONMENTS #################### -->

    <center><h1>Environments</h1></center><table align="center" width="1000px">
      <div class="container" style="width:800px; margin:0 auto;padding:5px" align="justify">
        <h2>Metaworld</h2>
        <p>The ablations that have been completed are done on the
          Metaworld benchmark, a benchmark for Meta and Multi-task RL algorithms.
          Here are visualizations of the different benchmarks available in Metaworld
          that were used in gathering the results for this project.
        </p>

        <div class="container">
          <h3>Meta-Learning 10 (ML10)</h3>
    
          <p>ML10 is a harder meta-learning task, where we train on 10 manipulation tasks, and are given 5 new ones at test time.</p>
          <img src="https://meta-world.github.io/figures/ml10.gif" class="img-fluid" alt="">
    
          <h3>Multi-Task 10 (MT10)</h3>
          <p>MT10 tests multi-task learning- that is, simply learning a policy that can succeed on a diverse set of tasks, without testing generalization. </p>
          <img src="https://meta-world.github.io/figures/mt10.gif" class="img-fluid" alt="">
          <div class="row">
            <div class="col-md-8 mx-auto">
              <!-- <h2 class="section-heading">Discover what all the buzz is about!</h2> -->
            </div>
          </div>
        </div>
  
      </div>


      </tbody></table>
    <br><hr>

    <center><h1>Experiments</h1></center><table align="center" width="1000px"></table>
       <div class="container" style="width:800px; margin:0 auto;padding:5px" align="justify">
        <h2>Reward Normalization</h2>
        <p>
          All of the environments that we try to train across have different 
          scales of reward: Some range from 0-1,000, some from 0-10, some 
          from 0-10,000. Increasing reward corresponds to improvement, <strong>the 
          same reward in 2 environments doesn’t imply the same amount of 
          improvement towards a task. The same optimizer is used to learn a 
          policy across these different reward functions.</strong> An optimizer would 
          fit a policy to whatever behaviors output the highest reward.
          What if we normalized our rewards so that they fall on <strong>similar 
          reward scales?</strong>
        </p>
        <img src="./demystifying_reproducibility_in_meta_mtrl_files/RL2_reward_normalization_ml10.png" class="img-fluid" alt="">
        <p>
          In the graph above, RL^2 with reward normalization is ablated against RL^2 without reward normalization. The curves
          show that adding reward normalization <strong> the performance of RL^2 by ~15% (p=0.04)</strong> on the benchmark ML10.
        </p>
       </div>
       <div class="container" style="width:800px; margin:0 auto;padding:5px" align="justify">
        <h2>Time Limit Terminations</h2>
        <p>
          Software implementers of RL algorithms frequently use a concept called 
          <a href="https://github.com/openai/gym/issues/1230">Time Limits</a>, 
          to express the idea that a task has a finite time-horizon. 
          In many open source trusted algorithm implementations 
          it is expressed by injecting a terminal state at the end of an episode.
          This has some nice properties when one implements RL algorithms in code,
          such as reducing the amount of code needed to express a sampler.
          <br>
          <br>
          The problem with this decision is that RL agents don’t have a concept of time. 
          Because the timestep number is normally not included as apart of the observation space,
          RL agents are unable to determine whether a terminal state is caused by an environment 
          terminating due to the task being solved or whether too many timesteps 
          collected in an environment without resetting it to its initial 
          configuration [time limit termination].
        </p>
        <img src="./demystifying_reproducibility_in_meta_mtrl_files/MTSAC_MT10_Time_Limits.png" class="img-fluid" alt="">
        <p>
          By removing time limits from the Metaworld MT10 benchmark, the <strong>performance of MTSAC increases 
          from ~30% to ~65%(p=0.05).</strong> This likely because as mentioned before, <strong>the addition of time limit 
          terminations turns MDPs into POMDPs.</strong> RL algorithms heavily strugle with optimizing POMDPs, especially off
          policy algorithms such as MTSAC.
        </p>
       </div>
       <div class="container" style="width:800px; margin:0 auto;padding:5px" align="justify">
        <h2>Max Entropy Rewards</h2>
        <img src="./demystifying_reproducibility_in_meta_mtrl_files/max_ent_reward.gif" class="img-fluid" alt="">
        <p>
          Adding the entropy of the policy to the reward outputted by the environment, is a trick that is used to 
          keep the width of the distributions outputted by policies wide. This result of this trick is best visualized
          by the below image.
        </p>
        <img src="./demystifying_reproducibility_in_meta_mtrl_files/max_entropy_explanation.png" class="img-fluid" alt="">
        <p>
          Making the distributions wider is a good action because it makes the exploration of the agent better in the
          long-term, which leads to faster convergence on an optimal policy. The problem is that differing amounts of 
          entropy are needed to solve the different tasks. High entropy is needed to solve difficult environments, 
          but makes it difficult to solve easy environments, and vice versa. This experiment ablates a max-entropy RL^2
          PPO against a vanilla RL^2 PPO and a max-entropy MTPPO against a vanilla MTPPO on the benchmarks ML10 and MT10
          respectively.
        </p>
        <img src="./demystifying_reproducibility_in_meta_mtrl_files/Entropy_maximization_ml10_mt10.png" class="img-fluid" alt="">
        <p>
          <strong>Max entropy hurts the performance of RL^2 PPO (P = 0.006) and MTPPO (P=0.04)</strong> <br>
          A max-entropy solution that caters to MTRL and meta-RL would be one that has an entropy per learned
          task.
        </p>
      </div>
      </div>

      <br><hr>
      <center><h1>Conclusion</h1></center><table align="center" width="1000px"></table>
      <div class="container" style="width:800px; margin:0 auto;padding:5px" align="justify">
      Small changes in the implementations of RL algorithms can make 
      significant differences in their performances. For any ML/DL/RL related 
      research projects, one should invest some time into reproducing correct, 
      high performing baselines. <br>
      Further work in meta-RL and MTRL should be more robust to not be dependent
      on these various tricks that were used to improve the performance of the 
      previously mentioned algorithms.
    </div>

      

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
</div>


<div id="fpCE_version" style="display:none">8.5.5</div></body></html>